{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeleccionDeModeloConfiguracionHiperparametros.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "PSdAxhrDlwdr",
        "outputId": "da11d4aa-4e53-4d57-c43b-49f21001464c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\u001b[0m\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Ign:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,298 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,302 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,521 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,871 kB]\n",
            "Fetched 10.2 MB in 3s (2,969 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "63 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 44 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 47.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=cf910cf92d753549ba46149c2fec34e7d9ab45f452326357df212d2166a6fcd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.2.1-bin-hadoop3.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#Bibliotecas para poder trabajar con Spark\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.1//spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "#Configuración de Spark con Python\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "\n",
        "#Estableciendo variable de entorno\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "#Buscando e inicializando la instalación de Spark\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creación de la SparkSession para trabajar\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('PySpark_prueba1').getOrCreate()"
      ],
      "metadata": {
        "id": "1_gfoq0tY33M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Validación cruzada (cross-validation)"
      ],
      "metadata": {
        "id": "sz6qGVRXZann"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargamos las bibliotecas necesarias para llevar acabo la validación cruzada\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "\n",
        "#Preparamos el conjunto de datos de entrenamiento en este caso cadenas de texto etiquetadas\n",
        "training = spark.createDataFrame([\n",
        "    (0, \"a b c d e spark\", 1.0),\n",
        "    (1, \"b d\", 0.0),\n",
        "    (2, \"spark f g h\", 1.0),\n",
        "    (3, \"hadoop mapreduce\", 0.0),\n",
        "    (4, \"b spark who\", 1.0),\n",
        "    (5, \"g d a y\", 0.0),\n",
        "    (6, \"spark fly\", 1.0),\n",
        "    (7, \"was mapreduce\", 0.0),\n",
        "    (8, \"e spark program\", 1.0),\n",
        "    (9, \"a e c l\", 0.0),\n",
        "    (10, \"spark compile\", 1.0),\n",
        "    (11, \"hadoop software\", 0.0)\n",
        "], [\"id\", \"text\", \"label\"])\n",
        "\n",
        "training .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZuMde2FTd1E",
        "outputId": "064de6f2-e543-4e68-f4d1-1c3bf0e44abe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+-----+\n",
            "| id|            text|label|\n",
            "+---+----------------+-----+\n",
            "|  0| a b c d e spark|  1.0|\n",
            "|  1|             b d|  0.0|\n",
            "|  2|     spark f g h|  1.0|\n",
            "|  3|hadoop mapreduce|  0.0|\n",
            "|  4|     b spark who|  1.0|\n",
            "|  5|         g d a y|  0.0|\n",
            "|  6|       spark fly|  1.0|\n",
            "|  7|   was mapreduce|  0.0|\n",
            "|  8| e spark program|  1.0|\n",
            "|  9|         a e c l|  0.0|\n",
            "| 10|   spark compile|  1.0|\n",
            "| 11| hadoop software|  0.0|\n",
            "+---+----------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creando un ML Pipeline"
      ],
      "metadata": {
        "id": "HP46R7F0gG5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración de un ML pipeline, que consiste en 3 fases: tokenizer, hashingTF, y regresión logística.\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "lr = LogisticRegression(maxIter=10)\n",
        "\n",
        "#creación del Pipeline\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
        "\n"
      ],
      "metadata": {
        "id": "fIe3XsmBZEPR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creación de la Grid de parámetros de configuración"
      ],
      "metadata": {
        "id": "BqCl8wGClB2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Se contruye un Grid de combinación de parámetros para cada esta del pipeline. \n",
        "#En este caso tres valores para hashingTF y 2 para la regresión logística.\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "    .build()"
      ],
      "metadata": {
        "id": "OTN6YN-Ck-FP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Se configura el objeto de validación cruzada y se ejecuta"
      ],
      "metadata": {
        "id": "U_o_cDsPlGAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Para crear la instancia de validación cruzada se toma el pipeline  y la Grid de parámetros.\n",
        "#Dependiendo del problema que se tenga se escoge el evaluador para este caso en particular dado\n",
        "#que es una regresión logística entonces es una clasificación binaria y aquí se coloca el número de\n",
        "#folds en los que se dividirá los datos iniciales\n",
        "\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=BinaryClassificationEvaluator(),\n",
        "                          numFolds=4)  # use 3+ folds in practice\n",
        "\n",
        "# Se ejecuta la validación cruzada para encontrar los mejores parámetros de configuración.\n",
        "cvModel = crossval.fit(training)"
      ],
      "metadata": {
        "id": "DzBy3ootlGIj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba del mejor modelo con documentos sin etiquetar"
      ],
      "metadata": {
        "id": "5uEteKvcmF5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se prepara documentos de prueba los cuales estan sin etiquetar.\n",
        "test = spark.createDataFrame([\n",
        "    (4, \"spark i j k\"),\n",
        "    (5, \"l m n\"),\n",
        "    (6, \"mapreduce spark\"),\n",
        "    (7, \"apache hadoop\")\n",
        "], [\"id\", \"text\"])\n",
        "\n",
        "# Se realizan las predicciones. cvModel usa el mejor modelo de regresión logistica encontrado dentro de la Grid de parámetros. (lrModel).\n",
        "prediction = cvModel.transform(test)\n",
        "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
        "for row in selected.collect():\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2u4dinKmJT7",
        "outputId": "f5902455-a2ee-4917-8162-329640f11344"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(id=4, text='spark i j k', probability=DenseVector([0.2665, 0.7335]), prediction=1.0)\n",
            "Row(id=5, text='l m n', probability=DenseVector([0.9204, 0.0796]), prediction=0.0)\n",
            "Row(id=6, text='mapreduce spark', probability=DenseVector([0.4438, 0.5562]), prediction=1.0)\n",
            "Row(id=7, text='apache hadoop', probability=DenseVector([0.8587, 0.1413]), prediction=0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train-Validation"
      ],
      "metadata": {
        "id": "I9bpbHudm0fE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Además del CrossValidator, Spark también ofrece TrainValidationSplit para el ajuste de hiperparámetros. TrainValidationSplit solo evalúa cada combinación de parámetros una vez, a diferencia de $k$ veces como CrossValidator. Por lo tanto, es menos costoso, pero no producirá resultados tan confiables cuando el conjunto de datos de entrenamiento no sea lo suficientemente grande. A diferencia de CrossValidator, TrainValidationSplit crea un único par de conjuntos de datos (entrenamiento, prueba). Divide el conjunto de datos en estas dos partes utilizando el parámetro *trainRatio*. Por ejemplo, con trainRatio=0.75, TrainValidationSplit generará un par de conjuntos de datos de prueba y entrenamiento donde el 75 % de los datos se usa para entrenamiento y el 25 % para validación."
      ],
      "metadata": {
        "id": "F9KVYM9Qm82i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "\n",
        "#Cargamos el conjunto de datos inicial dentro de la misma sesión\n",
        "data = spark.read.format(\"libsvm\")\\\n",
        "    .load(\"spark-3.2.1-bin-hadoop3.2/data/mllib/sample_linear_regression_data.txt\")\n",
        "train, test = data.randomSplit([0.9, 0.1], seed=12345)\n",
        "\n",
        "#Creamos el objeto base de regresión lineal\n",
        "lr = LinearRegression(maxIter=10)\n",
        "\n",
        "#Construimos la grid de búsqueda con los parámetros de configuración de la regresión lineal\n",
        "# regParam, fitIntercept y elasticNetParam\n",
        "paramGrid = ParamGridBuilder()\\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "    .addGrid(lr.fitIntercept, [False, True])\\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
        "    .build()\n",
        "\n",
        "#Como en el ejemplo anterior configuramos el validador\n",
        "tvs = TrainValidationSplit(estimator=lr,\n",
        "                           estimatorParamMaps=paramGrid,\n",
        "                           evaluator=RegressionEvaluator(),\n",
        "                           # 80% of the data will be used for training, 20% for validation.\n",
        "                           trainRatio=0.8)\n",
        "\n",
        "# Iniciamos la búsqueda de la mejor configuración\n",
        "model = tvs.fit(train)\n",
        "\n",
        "# Hacemos predicciones con los datos de prueba que no se han visto por el modelo.\n",
        "model.transform(test)\\\n",
        "    .select(\"features\", \"label\", \"prediction\")\\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztg53ANhTjpV",
        "outputId": "aa8f048f-2cf9-4f37-8551-e915ebd7e581"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|            features|               label|          prediction|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|(10,[0,1,2,3,4,5,...| -17.026492264209548|  -1.780062242348691|\n",
            "|(10,[0,1,2,3,4,5,...|  -16.71909683360509| -0.1893325701092588|\n",
            "|(10,[0,1,2,3,4,5,...| -15.375857723312297|  0.7252323736487188|\n",
            "|(10,[0,1,2,3,4,5,...| -13.772441561702871|  3.2696413241677718|\n",
            "|(10,[0,1,2,3,4,5,...| -13.039928064104615| 0.18817684046065775|\n",
            "|(10,[0,1,2,3,4,5,...|   -9.42898793151394|  -3.449987079269568|\n",
            "|(10,[0,1,2,3,4,5,...|    -9.2679651250406|-0.33109075490696316|\n",
            "|(10,[0,1,2,3,4,5,...|  -9.173693798406978|-0.42727135281551937|\n",
            "|(10,[0,1,2,3,4,5,...| -7.1500991588127265|   2.936884251408867|\n",
            "|(10,[0,1,2,3,4,5,...|  -6.930603551528371|-0.02839768193150...|\n",
            "|(10,[0,1,2,3,4,5,...|  -6.456944198081549| -0.9224776887934015|\n",
            "|(10,[0,1,2,3,4,5,...| -3.2843694575334834| -1.0821208483033875|\n",
            "|(10,[0,1,2,3,4,5,...|   -1.99891354174786|  0.8052068273813595|\n",
            "|(10,[0,1,2,3,4,5,...| -0.4683784136986876|  0.5046267770459569|\n",
            "|(10,[0,1,2,3,4,5,...|-0.44652227528840105|0.053072145020589406|\n",
            "|(10,[0,1,2,3,4,5,...| 0.10157453780074743| -1.0931313634366817|\n",
            "|(10,[0,1,2,3,4,5,...|  0.2105613019270259|   1.068404443672188|\n",
            "|(10,[0,1,2,3,4,5,...|  2.1214592666251364| 0.10226191630980659|\n",
            "|(10,[0,1,2,3,4,5,...|  2.8497179990245116|  1.1908709522287462|\n",
            "|(10,[0,1,2,3,4,5,...|   3.980473021620311|  2.3611397922073025|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}